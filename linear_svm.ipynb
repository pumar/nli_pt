{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import char_tokens\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globals\n",
    "test_size = 250\n",
    "bpath=os.getcwd() + r\"\\\\NLI_PT_v3\\NLI_PT_v3\\student\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents: 3069\n",
      "\n",
      "Statistics of size of documents (by number of characters): \n",
      "count     3069.000000\n",
      "mean      1045.513196\n",
      "std        818.494866\n",
      "min         23.000000\n",
      "25%        598.000000\n",
      "50%        923.000000\n",
      "75%       1336.000000\n",
      "max      19215.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def text_file_opener(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read().replace('\\n',' ')\n",
    "\n",
    "def parallel_extract():\n",
    "    texts = [ (text_file_opener(bpath +str(file))) for file in os.listdir(bpath)]\n",
    "    target = [language.split('_')[0] for language in os.listdir(bpath)]\n",
    "    return [texts,target]\n",
    "\n",
    "df = pd.DataFrame(data = list(map(list, zip(*parallel_extract()))),columns=[\"text\",\"lang\"])\n",
    "\n",
    "print(\"\\nNumber of documents: \" + str(df.shape[0]))\n",
    "print(\"\\nStatistics of size of documents (by number of characters): \" )\n",
    "print(str((df[\"text\"].str.len()).describe()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spa    614\n",
      "ita    565\n",
      "chi    445\n",
      "ger    438\n",
      "eng    415\n",
      "Name: lang, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.groupby(\"lang\").filter(lambda x: len(x) > 106)\n",
    "\n",
    "df = df.drop_duplicates(subset=\"text\")\n",
    "\n",
    "#Uses a token for names that were replaced in the texts with \"XXXXX\"\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"XXXXX\", \"ç¸µ\")\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "print(df[\"lang\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input layer size: 27685 features.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = char_tokens.NGramTokenizer(ngrams=3)\n",
    "tokenizer.text_fitting(df[\"text\"])\n",
    "\n",
    "print(\"\\nInput layer size: \" + str(len(tokenizer.ngram_dict)) + \" features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = np.array(tokenizer.tf_idf(df[\"text\"],'binary','idf'))\n",
    "df = df.drop(columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=1e-05, verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(tol=1e-5,max_iter=-1,verbose=1,class_weight=\"balanced\",gamma=\"scale\",kernel='linear')\n",
    "clf.fit(tfidf[:-1*test_size],df[\"lang\"].iloc[:-1*test_size])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (true positives/test set absolute size): \n",
      "0.704\n",
      "\n",
      "Confusion Matrix: \n",
      "[[42  7  2  4  3]\n",
      " [12 38  2  7  1]\n",
      " [ 2  1 37  2  0]\n",
      " [ 4  4  2 30  4]\n",
      " [ 4  4  6  3 29]]\n",
      "\n",
      "F1 Score:\n",
      "0.7022647361197628\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(df[\"lang\"].iloc[-1*test_size:],clf.predict(tfidf[-1*test_size:]),labels=[\"spa\",\"ita\",\"chi\",\"ger\",\"eng\"])\n",
    "print(\"\\nAccuracy (true positives/test set absolute size): \")\n",
    "print(np.trace(cm)/test_size)\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(cm)\n",
    "print('\\nF1 Score:')\n",
    "print(f1_score(df[\"lang\"].iloc[-1*test_size:],clf.predict(tfidf[-1*test_size:]),labels=[\"spa\",\"ita\",\"chi\",\"ger\",\"eng\"],average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
